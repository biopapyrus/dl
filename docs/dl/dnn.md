```{index} し 深層ニューラルネットワーク
:name: 深層ニューラルネットワーク
```

```{index} DNN
:name: DNN
```


# 深層ニューラルネットワーク


**深層ニューラルネットワーク**（**deep neural networks**; **DNN**）の歴史は、1940 年代に遡ります。1943 年、Warren McCulloch と Walter Pitts は、神経細胞の働きを数理モデルで表現した「McCulloch-Pitts モデル」を提案しました。このモデルは、ニューロンの発火や信号伝達を数学的に簡略化し、入出力を 0 または 1 に限定した単純な構造です {cite}`ref_nervous_model`。これがきっかけとなり、人間の脳のように情報を処理し学習する「ニューラルネットワーク」の研究が始まりました。

1960 年代には、Frank Rosenblatt が**パーセプトロン**（**perceptron**）を提案しました。このモデルは、簡単な分類問題を解く能力を持ち、当初は大きな期待を集めました。しかし、1969 年に Marvin Minsky と Seymour Papert が出版した『Perceptrons: An Introduction to Computational Geometry』{cite:p}`ref_perceptron_book`で、パーセプトロンの限界が明らかになります。このモデルでは非線形なデータを分離できないため、複雑な問題を解くことができませんでした。また、多層化による性能向上の可能性には言及されていたものの、当時の技術では多層パーセプトロンを効率的に訓練する方法が確立されていませんでした。この結果、ニューラルネットワーク研究は停滞し、ルールベースのエキスパートシステムなどが主流となりました。

1970 年代に入り、ニューラルネットワーク研究は再び注目を集めます。特に、**多層パーセプトロン**（**multilayer perceptron**; **MLP**）の概念と、その学習を効率化する**誤差逆伝播法**（**backpropagation**）の導入が大きな進展となりました。誤差逆伝播法は 1974 年に Paul Werbos によって提案され、その後、David E. Rumelhart、Geoffrey E. Hinton、Ronald J. Williams らによって改良され、1980 年代に広く普及しました {cite}`ref_propagation_history`。また、この時期に、畳み込みニューラルネットワークの基盤となる**ネオコグニトロン**（**Neocognitron**）や、データを自動分類する**自己組織化マップ**（**self-organizing map**; **SOM**）といった革新的なモデルも提案されました。しかし、この時代のコンピュータ性能はまだ十分ではなく、大規模なデータも不足していたため、ニューラルネットワークが実際に使われるケースは限られていました。そのため、この頃から**サポートベクターマシン**（**support-vector machine**; **SVM**）や**決定木**（**decision tree**）などの機械学習手法が主流となりました。

2000 年代後半になると、技術革新が相次ぎ、ニューラルネットワークは**深層学習**（**deep learning**）として再び注目を集めるようになります。この時期、GPU（グラフィックス処理装置）が汎用的に使われるようになり、膨大な計算を短時間で実行できるようになりました。また、インターネットの普及によって大量の画像やテキストデータが収集され、ImageNet {cite}`ref_imagenet`などの大規模なデータセットが構築されました。2006 年には Geoffrey Hinton らが**ディープビリーフネットワーク**（**deep belief network**; **DBN**）を提案し、層ごとに事前学習を行うことで、従来は困難だった深いネットワークの訓練を可能にしました{cite}`ref_dbn`。これらの技術革新が重なり、ニューラルネットワーク研究が新たな局面を迎えます。

2012 年には、Geoffrey Hinton の研究室が開発した AlexNet {cite}`ref_alexnet` が、ImageNet コンペティション{cite}`ref_ilsvrc`で従来手法を大きく上回る精度を達成しました。これが深層学習の可能性を広く示すきっかけとなり、画像処理や自然言語処理など多くの分野での応用が進みます。この頃から、画像認識に用いられる**畳み込みニューラルネットワーク**（**convolutional neural network**; **CNN**）や自然言語処理に用いられる**再帰型ニューラルネットワーク**（**recurrent neural network**; **RNN**）などの研究開発が高度化します。2014 年には**生成敵対ネットワーク**（**generative adversarial networks**; **GAN**）が提案され、画像生成や新しいデータ合成が可能になりました {cite}`ref_gan`。また、2016 年には、AlphaGo が囲碁のトップ棋士を破る快挙を達成し、強化学習との組み合わせの可能性を示しました。近年では、BERT、GPT-4 や LLaMA などの大規模モデルが登場し、自然言語処理や画像生成、創造的タスクで重要な役割を果たしています。

深層ニューラルネットワークは、80年以上の進化を経て、単純な数理モデルから複雑で高度な技術へと発展しました。その進歩を支えたのは、理論的革新、計算能力の向上、そしてデータの大規模化です。現在、深層学習は人工知能の中核を成し、新たな課題への挑戦を通じてさらに進化を続けるでしょう。


## 参照文献

```{bibliography} ../references.bib
:filter: docname in docnames
:style: plain
```

