{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from config import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習：内視鏡画像ポリープ領域分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内視鏡画像からポリープのある部位を正常な組織から明確に分離し、正確に特定することで、ポリープ除去手術や治療の精度を向上させることができます。このような領域の特定にはセグメンテーション技術が欠かせません。本節では、セマンティックセグメンテーション手法の一つである DeepLab V3 を用いて、ポリープ領域を抽出する方法を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本節で必要なライブラリを読み込みます。NumPy、Pandas、Matplotlib、Pillow（PIL）、OpenCV（cv2）は訓練過程の可視化や推論結果の表示に利用します。scikit-image（skimage）はマスクから輪郭線を計算する際に使用します。さらに、torch、torchvision、torchmetrics はインスタンスセグメンテーションモデルの訓練、検証、推論に利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import PIL\n",
    "\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.v2\n",
    "\n",
    "print(f'torch v{torch.__version__}; torchvision v{torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリの読み込み時に *ImportError* や *ModuleNotFoundError* が発生した場合は、該当するライブラリをインストールしてください。ライブラリのバージョンを揃える必要はありませんが、PyTorch（torch）および torchvision が上記のバージョンと異なる時、実行中に警告メッセージが現れたり、同じ結果にならなかったりする可能性があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章では、[Kvasir データセット](https://datasets.simula.no/kvasir/)[^kvasir_dataset]を使用します。このデータセットは内視鏡画像を集めた医療用データセットで、[Simula Research Laboratory](https://datasets.simula.no/) にて公開されています。Kvasir データセットは研究および教育目的に限り利用可能で、それ以外の用途での使用は許可されていません[^kvasir_termsofuse]。データセットを扱う際は、利用規約を必ず遵守してください。\n",
    "\n",
    "オリジナルの Kvasir データセットでは、セグメンテーション用の画像にはポリープを含むもののみが収録されています。しかし、実際の医療現場では、ポリープが存在しない健常者の画像も含まれることが一般的です。そこで、本演習では、オリジナルの Kvasir セグメンテーション用データから一部を抽出し、そこに Kvasir 分類用データから健常者の内視鏡画像を追加して、新しいデータセットを作成しました。\n",
    "\n",
    "新しいデータセットは、訓練データ 120 枚、検証データ 30 枚、テストデータ 30 枚で構成されています。訓練データは 100 枚の画像がポリープを持つ画像であり、残りの 20 枚は健全な画像です。また、検証データとテストデータはそれぞれ 20 枚のポリープ画像と 10 枚の健全画像が含まれています。\n",
    "\n",
    "```{figure} ../_static/kvasir_detection_dataset.jpg\n",
    "---\n",
    "name: kvasir_detection_dataset_example\n",
    "---\n",
    "Kvasir データセットに含まれる各カテゴリのサンプル画像。\n",
    "```\n",
    "\n",
    "Jupyter Notebook 上では、次のコマンドを実行することでダウンロードできます。\n",
    "\n",
    "```bash\n",
    "!wget https://dl.biopapyrus.jp/data/kvasirdet.zip\n",
    "!unzip kvasirdet.zip\n",
    "```\n",
    "\n",
    "\n",
    "[^kvasir_dataset]: Pogorelov et al. (2017) KVASIR: A Multi-Class Image Dataset for Computer Aided Gastrointestinal Disease Detection. *Proceedings of the 8th ACM on Multimedia Systems Conference*, [10.1145/3083187.3083212](https://doi.org/10.1145/3083187.3083212)\n",
    "\n",
    "[^kvasir_termsofuse]: \"The use of the Kvasir dataset is restricted for research and educational purposes only.\" [simula Kvasir](https://datasets.simula.no/kvasir/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "セマンティックセグメンテーションモデルを学習させるには、画像だけでなく、画像内のどこにどのようなオブジェクトが存在するかを示すマスクも同時に入力する必要があります。本節で使用する Kvasir データセットでは、アノテーションが COCO フォーマットで提供されています。そのため、この COCO フォーマットのアノテーションを適切に変換し、PyTorch が扱える形式であるテンソルに変換する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, root, annFile, image_size=(512, 512)):\n",
    "        super(CocoDataset, self).__init__(root, annFile)\n",
    "        self.image_size = image_size\n",
    "        self.transforms = torchvision.transforms.v2.Compose([\n",
    "            torchvision.transforms.v2.ToDtype(torch.float, scale=True),\n",
    "            torchvision.transforms.v2.ToPureTensor()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, target = super(CocoDataset, self).__getitem__(idx)\n",
    "\n",
    "        labels = []\n",
    "        boxes = []\n",
    "        masks = []\n",
    "        for obj in target:\n",
    "            bbox = obj['bbox']\n",
    "            bbox = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n",
    "            mask = PIL.Image.new('L', image.size, 0)\n",
    "            for poly in obj['segmentation']:\n",
    "                if isinstance(poly, list):\n",
    "                    PIL.ImageDraw.Draw(mask).polygon(poly, outline=1, fill=1)\n",
    "                else:\n",
    "                    rle_mask = Image.fromarray((pycocotools.mask.decode(rle) * 255).astype(np.uint8))                    \n",
    "                    mask = PIL.Image.composite(rle_mask, mask, rle_mask)\n",
    "            labels.append(obj['category_id'])\n",
    "            boxes.append(bbox)\n",
    "            masks.append(torch.tensor(np.array(mask), dtype=torch.float32))\n",
    "\n",
    "        if len(boxes) == 0: # dumy mask for non-polyp image\n",
    "            boxes.append([0, 0, 1, 1])\n",
    "            labels.append(0)\n",
    "            masks.append(torch.tensor(np.array(PIL.Image.new('L', image.size, 0)), dtype=torch.float32))\n",
    "        \n",
    "        \n",
    "        image = torchvision.transforms.Resize(self.image_size)(torchvision.transforms.functional.to_tensor(image))\n",
    "        # convert masks to a single mask\n",
    "        masks = [torchvision.transforms.Resize(self.image_size)(mask.unsqueeze(0)) for mask in masks] \n",
    "        masks = torch.stack(masks).squeeze(1)\n",
    "        mask_combined = torch.zeros(self.image_size, dtype=torch.long)\n",
    "        for i, mask in enumerate(masks):\n",
    "            mask_combined[mask > 0] = labels[i]\n",
    "\n",
    "        target = {\n",
    "            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.as_tensor(labels, dtype=torch.int64),\n",
    "            'masks': mask_combined,\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "def calculate_iou(pred_mask, true_mask, num_classes=2):\n",
    "    ious = []\n",
    "    for i in range(num_classes):\n",
    "        pred = (pred_mask == i).cpu().numpy().flatten()\n",
    "        true = (true_mask == i).cpu().numpy().flatten()\n",
    "        ious.append(sklearn.metrics.jaccard_score(true, pred))\n",
    "    return np.mean(ious)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算デバイス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算デバイスを設定します。PyTorch が GPU を認識できる場合は GPU を利用し、認識できない場合は CPU を利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepLab V3 のアーキテクチャは、torchvision.models.segmentation モジュールで提供されています。しかし、このモジュールに含まれるアーキテクチャは、車や人など90種類の一般的なオブジェクトを対象としているため、そのままではポリープのセマンティックセグメンテーションには適用できません。この問題に対処するには、DeepLab V3 の出力層のユニット数を変更する必要があります。ただし、この修正をモデルを呼び出すたびに行うのは手間がかかります。そこで、指定されたクラス数に応じたアーキテクチャを生成し、必要に応じて出力層を修正する処理を関数として定義します。\n",
    "\n",
    "なお、セマンティックセグメンテーションでは、物体検出と同様に背景も 1 つのクラスとして扱います。そのため、出力層のユニット数を変更する際には、セグメンテーション対象のクラス数に 1 を加えた数に設定する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "def deeplabv3(num_classes, weights=None):\n",
    "    num_classes = num_classes + 1\n",
    "    model = torchvision.models.segmentation.deeplabv3_resnet50(weights='DEFAULT')\n",
    "    model.classifier[4] = torch.nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "    if weights is not None:\n",
    "        model.load_state_dict(torch.load(weights))\n",
    "    return model\n",
    "\n",
    "model = deeplabv3(1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練を開始する前に、モデルのパラメータを最適化するためのアルゴリズム（`optimizer`）と、学習率（`lr`）を動的に調整するスケジューラ（`lr_scheduler`）を設定します。セマンティックセグメンテーションでは、各ピクセルごとにクラス分類を行うため、損失関数（`criterion`）として多クラス分類で一般的に使用される交差エントロピー関数を採用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、訓練サブセットおよび検証サブセットを読み込み、モデルに入力できる形式に整えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    CocoDataset('kvasirdet/train', 'kvasirdet/train/segm.json'),\n",
    "                    batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "                    CocoDataset('kvasirdet/valid', 'kvasirdet/valid/segm.json'),\n",
    "                    batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備が整ったら、訓練を開始します。訓練プロセスでは、訓練と検証を繰り返します。訓練では、訓練データを使用してモデルのパラメータを更新し、訓練データにおける損失（誤差）を記録します。また、検証では検証データを利用してモデルの予測性能（IoU）を計算し記録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs= 10\n",
    "metric_dict = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    metric_dict_ = {'epoch': epoch + 1, 'train_loss': 0.0, 'valid_iou': 0.0}\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    n_trains = 0\n",
    "    for images, targets in train_loader:\n",
    "        images = torch.stack([img.to(device) for img in images])\n",
    "        masks = torch.stack([t['masks'].long().to(device) for t in targets])\n",
    "        \n",
    "        outputs = model(images)\n",
    "        batch_loss = criterion(outputs['out'], masks)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        metric_dict_['train_loss'] += batch_loss.item()\n",
    "        n_trains += len(targets)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    n_valids = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in valid_loader:\n",
    "            images = torch.stack([img.to(device) for img in images])\n",
    "            masks =  torch.stack([t['masks'].long().to(device) for t in targets])\n",
    "            \n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs['out'], dim=1)\n",
    "            \n",
    "            for i in range(len(preds)):\n",
    "                metric_dict_['valid_iou'] +=  calculate_iou(preds[i], masks[i])\n",
    "            n_valids += len(targets)\n",
    "\n",
    "    # compute avg. loss/IoU per sample\n",
    "    metric_dict_['train_loss'] /= n_trains\n",
    "    metric_dict_['valid_iou'] /= n_valids\n",
    "\n",
    "    metric_dict.append(metric_dict_)\n",
    "    print(metric_dict_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データに対する損失と検証データに対する予測性能（IoU）を可視化し、訓練過程を評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = pd.DataFrame(metric_dict)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].plot(metric_dict['epoch'], metric_dict['train_loss'])\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].set_title('Train')\n",
    "ax[1].plot(metric_dict['epoch'], metric_dict['valid_iou'])\n",
    "ax[1].set_ylim(0, 1)\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_ylabel('IoU')\n",
    "ax[1].set_title('Validation')\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可視化の結果から、エポック数が増加するにつれて訓練データに対する損失が継続的に減少していることが確認できます。特に、10 エポック目においても訓練損失が収束する兆候は見られません。一方で、検証データに対する予測性能（IoU）は、数エポックで最大値に達し、その後は収束しているように見受けられます。\n",
    "\n",
    "次に、この手順を U-Net など他の深層ニューラルネットワークのアーキテクチャに適用し、それぞれの検証性能を比較します。この比較によって、データセットに最も適したアーキテクチャを選定することが可能です。ただし、本節ではこの比較を省略し、DeepLab V3 を最適なアーキテクチャとして採用し、次のステップに進みます。\n",
    "\n",
    "次のステップでは、訓練サブセットと検証サブセットを統合し、選定した最適なアーキテクチャを用いてモデルを初めから再訓練します。その準備として、まず訓練サブセットと検証サブセットを結合します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!rm -rf kvasirdet/trainvalid/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir kvasirdet/trainvalid/images\n",
    "!cp kvasirdet/train/images/* kvasirdet/trainvalid/images\n",
    "!cp kvasirdet/valid/images/* kvasirdet/trainvalid/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、モデルの構築を行います。先ほど可視化した検証性能の推移グラフを確認した結果、数エポックの訓練で十分に高い予測性能を達成できることがわかりました。そこで、ここでは訓練サブセットと検証サブセットを統合したデータを用いて、5 エポックのみ訓練を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "model = deeplabv3(1)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    CocoDataset('kvasirdet/trainvalid', 'kvasirdet/trainvalid/segm.json'),\n",
    "                    batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "\n",
    "num_epochs= 5\n",
    "metric_dict = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    metric_dict_ = {'epoch': epoch + 1, 'train_loss': 0.0}\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    n_trains = 0\n",
    "    for images, targets in train_loader:\n",
    "        images = torch.stack([img.to(device) for img in images])\n",
    "        masks = torch.stack([t['masks'].long().to(device) for t in targets])\n",
    "        \n",
    "        outputs = model(images)\n",
    "        batch_loss = criterion(outputs['out'], masks)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        metric_dict_['train_loss'] += batch_loss.item()\n",
    "        n_trains += len(targets)\n",
    "\n",
    "    # compute metrics per sample\n",
    "    metric_dict_['train_loss'] /= n_trains\n",
    "    metric_dict.append(metric_dict_)\n",
    "    print(metric_dict_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練が完了したら、訓練済みモデルの重みをファイルに保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "torch.save(model.state_dict(), 'kvasirsegm.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論時にも、訓練時と同じように torchvision モジュールからアーキテクチャを呼び出し、出力層のクラス数を設定します。その後、`load_state_dict` メソッドを使って、訓練済みの重みファイルをモデルにロードします。これらの操作はすべて `deeplabv3` 関数で定義されているので、その関数を利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "model = deeplabv3(1, 'kvasirsegm.pth')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このモデルを利用して推論を行います。まず、1 枚の画像を指定し、PIL モジュールを用いて画像を開きます。その後、画像をテンソル形式に変換してモデルに入力します。モデルは予測結果として、分類対象のカテゴリ数と同じ数のマスクを生成します。これらのマスクについて、各画素位置で値を比較し、最も大きな値を持つマスクが何番目かを調べることで、その画素がどのクラスに属しているかを判定します。\n",
    "\n",
    "すべての画素について判定した結果を1枚のマスク（`pred_mask`）として保存します。このマスク（`pred_mask`）は入力画像と同じ解像度を持ち、要素は0、1、2、...といった整数で構成されます。ここで、0 の領域は背景を表し、1 の領域は ID が 1 のオブジェクトを、2 の領域は ID が 2 のオブジェクトを示すようになっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "image_path = 'kvasirdet/test/images/cju2lz8vqktne0993fuym6drw.jpg'\n",
    "\n",
    "image = PIL.Image.open(image_path).convert('RGB')\n",
    "input_tensor = torchvision.transforms.functional.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_tensor)['out']\n",
    "\n",
    "pred_mask = torch.argmax(predictions, dim=1).squeeze(0).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本節では、ポリープの検出のみを行っているため、生成されたマスクは 0（背景）または 1（ポリープ）となっています。このため、1 の領域を緑色に染めて可視化してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask = (pred_mask == 1).astype(np.uint8)\n",
    "image = np.array(image)\n",
    "\n",
    "mask_overlay = np.zeros_like(image)\n",
    "mask_overlay[pred_mask == 1] = [0, 255, 0]\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "mask_overlay = cv2.cvtColor(mask_overlay, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "overlayed_image = cv2.addWeighted(image, 0.7, mask_overlay, 0.3, 0)\n",
    "overlayed_image = cv2.cvtColor(overlayed_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(overlayed_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
