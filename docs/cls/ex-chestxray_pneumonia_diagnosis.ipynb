{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if os.path.exists('../ebookconfig.py'):\n",
    "    sys.path.append(os.path.abspath('..'))\n",
    "    from ebookconfig import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習：胸部 X 線画像診断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深層学習を活用した胸部 X 線画像診断支援システムは、医療分野で注目されている技術です。この技術は、X 線画像を解析し、肺炎や肺がんなどの病変を検出することで、診断をサポートすることを目的としています。医師の診断を補助することで、小さな病変を見逃すリスクを減らし、診断の迅速化や医師の業務負担の軽減に貢献できると期待されています。本節では、深層学習を使って健康者と肺炎患者の胸部 X 線画像を解析し、肺炎の有無を診断するプログラムを作成する方法を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本節で利用するライブラリを読み込みます。ライブラリの読み込み時に *ImportError* や *ModuleNotFoundError* が発生した場合は、該当するライブラリをインストールしてください。pytorch_grad_cam を読み込むときに ModuleNotFoundError が発生した場合は、[grad-cam](https://github.com/jacobgil/pytorch-grad-cam) パッケージをインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "\n",
    "# machine learning\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# grad-CAM visualization\n",
    "import cv2\n",
    "import pytorch_grad_cam\n",
    "\n",
    "print(f'torch v{torch.__version__}; torchvision v{torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本節では、Kermany らの論文[^kermany_2018]で公開されている胸部 X 線画像データセットを利用します。このデータセットには、健康者（NORMAL）と肺炎患者（PNEUMONIA）の胸部 X 線画像が含まれています。また、肺炎患者の画像は、さらに細菌性肺炎とウイルス性肺炎の 2 種類に分類されています（{numref}`fig-chestxray_pneumonia_dataset_example`）。\n",
    "\n",
    "```{figure} ../_static/chestxray_pneumonia_dataset.jpg\n",
    "---\n",
    "name: fig-chestxray_pneumonia_dataset_example\n",
    "---\n",
    "Kermany らのデータセットに含まれる健康者（normal）、細菌性肺炎患者（bacteria）、ウイルス性肺炎患者（virus）の胸部 X 線画像の例。\n",
    "```\n",
    "\n",
    "このデータセットは、CC-BY 4.0 ライセンスのもとで [Mendeley Data](https://doi.org/10.17632/rscbjbr9sj.2) に公開されています。このライセンスにより、著作権表示を行うことで、編集や再配布を含む自由な利用が可能です。\n",
    "\n",
    "本節では、オリジナルのデータセットからランダムに抽出した 360 枚の画像を使用して作成した小規模なデータセットを利用します。このデータセットは、訓練サブセット、検証サブセット、テストサブセットの 3 つに分かれています。訓練サブセットには各カテゴリ（normal、bacteria、virus）の画像がそれぞれ 100 枚含まれており、検証サブセットとテストサブセットには、それぞれ各カテゴリ 10 枚の画像が含まれています。\n",
    "\n",
    "本節で利用するデータセットは、Jupyter Notebook 上では、次のコマンドを実行することでダウンロードできます。\n",
    "\n",
    "\n",
    "```bash\n",
    "!wget https://dl.biopapyrus.jp/data/chestxray_pneumonia.zip\n",
    "!unzip chestxray_pneumonia.zip\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "[^kermany_2018]: Kermany et al. (2018) Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. **Cell**, [10.1016/j.cell.2018.02.010](https://doi.org/10.1016/j.cell.2018.02.010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みニューラルネットワークは、ニューロンの数などが固定されているため、入力する画像のサイズにも制限があります。例えば、本節で使用する EfficientNet V2（efficientnet_v2_m） [^efficientnet_v2_m_input] では、480&times;480 ピクセルの正方形画像を入力として設計されています。また、PyTorch ではすべてのデータをテンソル形式で扱う必要があります。そのため、畳み込みニューラルネットワークに画像を入力する前に、画像サイズを適切に調整し、テンソル型に変換するといった前処理を行う必要があります。以下では、この前処理の手順を定義します。\n",
    "\n",
    "\n",
    "[^efficientnet_v2_m_input]: https://pytorch.org/vision/0.16/models/generated/torchvision.models.efficientnet_v2_m.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareResize:\n",
    "    def __init__(self, shape=480, bg_color = (0, 0, 0)):\n",
    "        self.shape = shape\n",
    "        self.bg_color = tuple(bg_color)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        img_square = None\n",
    "\n",
    "        if w == h:\n",
    "            img_square = img\n",
    "        elif w > h:\n",
    "            img_square = PIL.Image.new(img.mode, (w, w), self.bg_color)\n",
    "            img_square.paste(img, (0, (w - h) // 2))\n",
    "        else:\n",
    "            img_square = PIL.Image.new(img.mode, (h, h), self.bg_color)\n",
    "            img_square.paste(img, ((h - w) // 2, 0))\n",
    "\n",
    "        img_square = img_square.resize((self.shape, self.shape))\n",
    "        return img_square\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    SquareResize(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算デバイス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算を行うデバイスを設定します。PyTorch が GPU を認識できる場合は GPU を利用し、認識できない場合は CPU を使用するように設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EfficientNet V2 のアーキテクチャを利用してモデルを構築します。torchvision.models モジュールで提供されている [EfficientNet V2]((https://pytorch.org/vision/main/models/efficientnetv2.html)) には、efficientnet_v2_s、efficientnet_v2_m、efficientnet_v2_l の 3 つのサブタイプがあります。それぞれ、入力画像のサイズが異なり、入力画像サイズが大きいほど性能が向上しますが、モデルのパラメータが多くなり、訓練時間も長くなります。本節では、efficientnet_v2_m を利用します。このアーキテクチャは、480&times;480 ピクセルの画像を入力として設計されています。\n",
    "\n",
    "torchvision.models モジュールで提供されているアーキテクチャは、飛行機や車、人など、1000 種類の一般的な物体を分類するように設計されています。これに対して、本節では、normal、bacteria、virus の 3 カテゴリの分類問題を扱います。そのため、torchvision.models モジュールから読み込んだ EfficientNet V2 の出力層のユニット数を 3 に変更する必要があります。この修正作業はモデルを呼び出すたびに行う必要があり、手間がかかります。そこで、一連の処理を関数化してから利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "def efficientnet_v2(num_classes, weights=None):\n",
    "    model = torchvision.models.efficientnet_v2_m(weights='DEFAULT')\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier[1] = torch.nn.Linear(in_features, num_classes)\n",
    "    if weights is not None:\n",
    "        model.load_state_dict(torch.load(weights))\n",
    "    return model\n",
    "\n",
    "model = efficientnet_v2(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルが学習データを効率よく学習できるようにするため、損失関数（`criterion`）、学習アルゴリズム（`optimizer`）、学習率（`lr`）、および学習率を調整するスケジューラ（`lr_scheduler`）を設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00003)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、訓練データと検証データを読み込み、モデルが入力できる形式に整えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder('chestxray_pneumonia/train', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "valid_dataset = torchvision.datasets.ImageFolder('chestxray_pneumonia/valid', transform=transform)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備が整ったら、訓練を開始します。訓練プロセスでは、訓練と検証を交互に繰り返します。訓練では、訓練データを使ってモデルのパラメータを更新し、その際の損失（誤差）を記録します。検証では、検証データを使ってモデルの予測性能（正解率）を計算し、その結果を記録します。このサイクルを繰り返すことで、モデルの精度を少しずつ向上させていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "num_epochs = 15\n",
    "metric_dict = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training phase\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    n_correct_train = 0\n",
    "    n_train_samples = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs.data, 1)\n",
    "        n_correct_train += torch.sum(predicted_labels == labels).item()\n",
    "        n_train_samples += labels.size(0)\n",
    "        running_loss +=  loss.item() / len(train_loader)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "\n",
    "    # validation phase\n",
    "    model.eval()\n",
    "    \n",
    "    n_correct_valid = 0\n",
    "    n_valid_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted_labels = torch.max(outputs.data, 1)\n",
    "            n_correct_valid += torch.sum(predicted_labels == labels).item()\n",
    "            n_valid_samples += labels.size(0)\n",
    "\n",
    "    metric_dict.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': running_loss,\n",
    "        'train_acc': n_correct_train / n_train_samples,\n",
    "        'valid_acc': n_correct_valid / n_valid_samples\n",
    "    })\n",
    "  \n",
    "    print(metric_dict[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データに対する損失と検証データに対する正解率を可視化し、訓練過程を評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "metric_dict = pd.DataFrame(metric_dict)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].plot(metric_dict['epoch'], metric_dict['train_loss'])\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].set_title('Train')\n",
    "ax[1].plot(metric_dict['epoch'], metric_dict['valid_acc'])\n",
    "ax[1].set_ylim(0, 1)\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].set_title('Validation')\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "if 'glue' in vars():\n",
    "    glue('valid_acc', metric_dict['valid_acc'].iloc[5:].mean(), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可視化の結果、エポックが増えるにつれて訓練データに対する損失が減少し、10 エポック前後で収束し始めたことが確認できました。また、検証データに対する分類性能（正解率）は、最初の数エポックからほぼ最大値に達しており、EfficientNet V2 は数エポックの学習だけで十分であることがわかります。\n",
    "\n",
    "次に、同じ手順を異なる深層ニューラルネットワークアーキテクチャ（DenseNet や Inception など）に対して実施し、それぞれのアーキテクチャの検証性能を比較します。そして、このデータセットに最適なアーキテクチャを選定します。ただし、本節ではモデル（アーキテクチャ）選択を行わずに、EfficientNet V2 を最適なアーキテクチャとして採用し、次のステップに進みます。\n",
    "\n",
    "次のステップでは、訓練サブセットと検証サブセットを統合し、最適なアーキテクチャを最初から訓練します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!rm -rf chestxray_pneumonia/trainvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir chestxray_pneumonia/trainvalid\n",
    "!cp -r chestxray_pneumonia/train/* chestxray_pneumonia/trainvalid\n",
    "!cp -r chestxray_pneumonia/valid/* chestxray_pneumonia/trainvalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適なモデルを選択する段階で、数エポックの訓練だけでも十分に高い予測性能を獲得できたことがわかったので、ここでは訓練サブセットと検証サブセットを統合したデータに対して 5 エポックだけ訓練させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = efficientnet_v2(3)\n",
    "model.to(device)\n",
    "\n",
    "# training params\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00003)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# training data\n",
    "train_dataset = torchvision.datasets.ImageFolder('chestxray_pneumonia/trainvalid', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# training\n",
    "num_epochs = 5\n",
    "metric_dict = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    n_correct_train = 0\n",
    "    n_train_samples = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs.data, 1)\n",
    "        n_correct_train += torch.sum(predicted_labels == labels).item()\n",
    "        n_train_samples += labels.size(0)\n",
    "        running_loss +=  loss.item() / len(train_loader)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    metric_dict.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': running_loss,\n",
    "        'train_acc': n_correct_train / n_train_samples,\n",
    "    })\n",
    "  \n",
    "    print(metric_dict[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練が完了したら、訓練済みモデルの重みをファイルに保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')\n",
    "torch.save(model.state_dict(), 'chestxray_pneumonia.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適なモデルが得られたら、次にテストデータを用いてモデルを詳細に評価します。正解率だけでなく、適合率、再現率、F1 スコアなどの評価指標を計算し、モデルを総合的に評価します。まず、テストデータをモデルに入力し、その予測結果を取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.ImageFolder('chestxray_pneumonia/test', transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "model = efficientnet_v2(3, 'chestxray_pneumonia.pth')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, _labels = torch.max(outputs.data, 1)\n",
    "        #print(_labels)\n",
    "        pred_labels.extend(_labels.cpu().detach().numpy().tolist())\n",
    "        true_labels.extend(labels.cpu().detach().numpy().tolist())\n",
    "\n",
    "pred_labels = [test_dataset.classes[_] for _ in pred_labels]\n",
    "true_labels = [test_dataset.classes[_] for _ in true_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、予測結果とラベルを比較し、混同行列を作成します。これにより、間違いやすいカテゴリを特定することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sklearn.metrics.confusion_matrix(true_labels, pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = sklearn.metrics.ConfusionMatrixDisplay(cm, display_labels=test_dataset.classes)\n",
    "cmp.plot(xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それぞれのクラスに対する適合率、再現率、F1 スコアなどは、scikit-learn ライブラリを利用して計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sklearn.metrics.classification_report(true_labels, pred_labels, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論を行う際には、訓練や評価時と同様に、torchvision.models モジュールから EfficientNet V2 のアーキテクチャを読み込み、出力層のクラス数を設定します。その後、`load_state_dict` メソッドを使用して訓練済みの重みファイルをモデルにロードします。これらの処理はすでに関数化（`efficientnet_v2`）されているため、その関数を利用して簡単に実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "labels = ['bacteria', 'normal', 'virus']\n",
    "\n",
    "model = efficientnet_v2(3, 'chestxray_pneumonia.pth')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このモデルを使用して推論を行います。細菌性肺炎（bacteria）の画像を 1 枚指定し、訓練時と同様の前処理を行います。その後、前処理した画像をモデルに入力し、モデルから予測結果が出力されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'chestxray_pneumonia/test/bacteria/person5_bacteria_16.jpeg'\n",
    "\n",
    "image = PIL.Image.open(image_path).convert('RGB')\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    score = model(input_tensor)[0]\n",
    "\n",
    "output = pd.DataFrame({\n",
    "    'class': labels,\n",
    "    'probability': torch.softmax(score, axis=0).cpu().detach().numpy() \n",
    "})\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、別の例を見てみましょう。ウィルス性肺炎（virus）の画像をモデルに入力し、推論を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "image_path = 'chestxray_pneumonia/test/virus/person111_virus_212.jpeg'\n",
    "\n",
    "image = PIL.Image.open(image_path).convert('RGB')\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    score = model(input_tensor)[0]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'class': labels,\n",
    "    'probability': torch.softmax(score, axis=0).cpu().detach().numpy() \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 分類根拠の可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みニューラルネットワークを用いた画像分類では、畳み込み層で抽出された特徴マップが分類に大きな影響を与えています。そのため、最後の畳み込み層で得られた特徴マップと、それに対応する重みを可視化することで、モデルがどの部分に注目して分類を行ったのか、つまり判断の根拠を明確にすることができます。\n",
    "\n",
    "本節では、Grad-CAM（Gradient-weighted Class Activation Mapping）および Guided Grad-CAM という手法を用いて、モデルの判断根拠を可視化します。可視化には Python の grad-cam パッケージを使用します。必要に応じてインストールし、grad-cam の[チュートリアル](https://jacobgil.github.io/pytorch-gradcam-book/introduction.html)を参考にしながら、Grad-CAM および Guided Grad-CAM を計算し、可視化するための関数を定義します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def viz(image_path):\n",
    "    labels = ['bacteria', 'normal', 'virus']\n",
    "    \n",
    "    model = efficientnet_v2(3, 'chestxray_pneumonia.pth')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # load image\n",
    "    image = PIL.Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    rgb_img = cv2.imread(image_path, 1)[:, :, ::-1]\n",
    "    rgb_img = np.float32(np.array(SquareResize()(image))) / 255\n",
    "    \n",
    "    # Grad-CAM\n",
    "    with pytorch_grad_cam.GradCAM(model=model, target_layers=[model.features[8]]) as cam:\n",
    "        cam.batch_size = 8\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=None,aug_smooth=True, eigen_smooth=True)\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        cam_image = pytorch_grad_cam.utils.image.show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "        prob = torch.softmax(cam.outputs[0], axis=0).cpu().detach().numpy()\n",
    "\n",
    "    gb_model = pytorch_grad_cam.GuidedBackpropReLUModel(model=model, device=device)\n",
    "    gb = gb_model(input_tensor, target_category=None)\n",
    "    cam_mask = np.stack([grayscale_cam, grayscale_cam, grayscale_cam], axis=-1)\n",
    "    cam_gb = pytorch_grad_cam.utils.image.deprocess_image(cam_mask * gb)\n",
    "    gb = pytorch_grad_cam.utils.image.deprocess_image(gb)\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = plt.subplots(2, 2)\n",
    "    ax[0, 0].imshow(rgb_img)\n",
    "    ax[0, 0].axis('off')\n",
    "    ax[0, 0].set_title('Original Image', fontsize=16)\n",
    "    ax[0, 1].imshow(cam_image)\n",
    "    ax[0, 1].axis('off')\n",
    "    ax[0, 1].set_title('Grad-CAM', fontsize=16)\n",
    "    ax[1, 0].imshow(gb)\n",
    "    ax[1, 0].axis('off')\n",
    "    ax[1, 0].set_title('Guided Backpropagation', fontsize=16)\n",
    "    ax[1, 1].imshow(cam_gb)\n",
    "    ax[1, 1].axis('off')\n",
    "    ax[1, 1].set_title('Guided Grad-CAM', fontsize=16)\n",
    "    print(pd.DataFrame({'class': labels, 'probability': prob}))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "次に、いくつかの画像をこの可視化関数に入力し、モデルの予測結果とその判断根拠を可視化します。これにより、モデルがどの部分に注目して分類を行ったのかを視覚的に確認することができます。必要に応じて、他の画像を入力し、それぞれの分類結果と判断根拠を可視化してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "viz('chestxray_pneumonia/test/bacteria/person5_bacteria_16.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "viz('chestxray_pneumonia/test/virus/person1365_virus_2348.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!rm -rf chestxray_pneumonia/trainvalid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
