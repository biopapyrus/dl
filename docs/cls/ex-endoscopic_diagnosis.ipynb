{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from config import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習：内視鏡画像診断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内視鏡を利用した胃がん検査は、X 線検査に比べて精度が高く、がんの早期発見に大きく貢献しています。しかし、内視鏡検査では医師が画像を確認しながら診断を行う必要があり、その作業負担が非常に大きいという課題があります。近年、人工知能（AI）を活用した疾患部位の自動検出技術が進展しており、これにより医師の負担軽減や診断の効率化が期待されています。本節では、深層ニューラルネットワークを用いて内視鏡画像から胃腸疾患を診断するプログラムの開発方法を学びます。この演習を通じて、深層学習を活用した診断支援の可能性を具体的に体験し、実際の医療応用を意識した知識とスキルを深めることを目指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本節で利用するライブラリを読み込みます。NumPy、Pnadas、Matplotlib、[Pillow](https://pillow.readthedocs.io/)（PIL）などのライブライは、モデルの性能や推論結果などの可視化に利用します。[scikit-learn](https://scikit-learn.org/)（sklearn）、[PyTorch](https://pytorch.org/)（torch）、torchvision は機械学習関連のライブラリであり、モデルの構築、検証や推論などに利用します。また、OpenCV（cv2）、pytorch_grad_cam（[grad-cam](https://github.com/jacobgil/pytorch-grad-cam)）などはモデルの推論根拠を可視化するために利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "\n",
    "# machine learning\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# grad-CAM visualization\n",
    "import cv2\n",
    "import pytorch_grad_cam\n",
    "\n",
    "print(f'torch v{torch.__version__}; torchvision v{torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリの読み込み時に *ImportError* や *ModuleNotFoundError* が発生した場合は、該当するライブラリをインストールしてください。ライブラリのバージョンを揃える必要はありませんが、PyTorch（torch）および torchvision が上記のバージョンと異なる時、実行中に警告メッセージが現れたり、同じ結果にならなかったりする可能性があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本節では、[Simula Research Laboratory](https://datasets.simula.no/) によって公開されている Kvasir データセット[^kvasir_dataset]を使用します。このデータセットは、内視鏡画像を集めた医療用データセットであり、研究および教育目的に限り利用が許可されています[^kvasir_termsofuse]。\n",
    "\n",
    "Kvasir データセットは 8 つのカテゴリに分類されていますが、本節ではその中の健全な盲腸（normal-cecum）、健全な幽門（normal-pylorus）、健全な食道胃粘膜移行帯（normal-z-line）、食道炎（esophagitis）、潰瘍性大腸炎（ulcerative-colitis）、ポリープ（polyps）の 6 カテゴリを対象に取り扱います（{numref}`fig-kvasir_classification_dataset_example`）。\n",
    "\n",
    "\n",
    "```{figure} ../_static/kvasir_classification_dataset.jpg\n",
    "---\n",
    "name: fig-kvasir_classification_dataset_example\n",
    "---\n",
    "Kvasir データセットに含まれる各カテゴリのサンプル画像。\n",
    "```\n",
    "\n",
    "本節では、プログラムを短時間で実行できるようにするため、オリジナルの Kvasir データセットから各カテゴリごとにランダムで 100 枚の訓練画像、20 枚の検証画像、20 枚のテスト画像を抽出し、小規模なデータセットを作成して使用します。Jupyter Notebook 上では、以下のコマンドを実行することでデータセットをダウンロードできます。\n",
    "\n",
    "```bash\n",
    "!wget https://dl.biopapyrus.jp/data/kvasir.zip\n",
    "!unzip kvasir.zip\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "[^kvasir_dataset]: Pogorelov et al. (2017) KVASIR: A Multi-Class Image Dataset for Computer Aided Gastrointestinal Disease Detection. *Proceedings of the 8th ACM on Multimedia Systems Conference*, [10.1145/3083187.3083212](https://doi.org/10.1145/3083187.3083212)\n",
    "\n",
    "[^kvasir_termsofuse]: \"The use of the Kvasir dataset is restricted for research and educational purposes only.\" [simula Kvasir](https://datasets.simula.no/kvasir/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みニューラルネットワークは、ニューロンの数などが固定されているため、入力する画像のサイズにも制限があります。例えば、本節で使用する DenseNet 121 [^densenet121_input] では、224&times;224 ピクセルの正方形画像を入力として設計されています。また、PyTorch ではすべてのデータをテンソル形式で扱う必要があります。そのため、畳み込みニューラルネットワークに画像を入力する前に、画像サイズを適切に調整し、テンソル型に変換するといった前処理を行う必要があります。以下では、この前処理の手順を定義します。\n",
    "\n",
    "[^densenet121_input]: https://pytorch.org/vision/main/models/generated/torchvision.models.densenet121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareResize:\n",
    "    def __init__(self, shape=224, bg_color = (0, 0, 0)):\n",
    "        self.shape = shape\n",
    "        self.bg_color = tuple(bg_color)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        img_square = None\n",
    "\n",
    "        if w == h:\n",
    "            img_square = img\n",
    "        elif w > h:\n",
    "            img_square = PIL.Image.new(img.mode, (w, w), self.bg_color)\n",
    "            img_square.paste(img, (0, (w - h) // 2))\n",
    "        else:\n",
    "            img_square = PIL.Image.new(img.mode, (h, h), self.bg_color)\n",
    "            img_square.paste(img, ((h - w) // 2, 0))\n",
    "\n",
    "        img_square = img_square.resize((self.shape, self.shape))\n",
    "        return img_square\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    SquareResize(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像データは通常、0 から 255 の範囲の整数値で表現されていますが、前処理の段階でこれを正規化します。正規化により、画像データの値は平均約 0.5、標準偏差約 0.23 の範囲に変換され、モデルの学習を効率的に進めることができます。なお、正規化の際に平均を 0.50、分散を 0.23 のような切りの良い数値にしない理由は、これから利用する torchvision.models が提供する訓練済みモデルが、特定の数値（例えば平均 0.485、標準偏差 0.229）で訓練されているためです。そのため、この訓練済みモデルに合わせて正規化を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算デバイス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算を行うデバイスを設定します。PyTorch が GPU を認識できる場合は GPU を利用し、認識できない場合は CPU を使用するように設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本節では、物体分類のアーキテクチャとして DenseNet 121 を使用します。このアーキテクチャは、深い層を持ちながらもパラメータ数を大幅に削減した設計が特徴です。\n",
    "\n",
    "torchvision.models モジュールで提供されている DenseNet 121 は、飛行機や車、人など、1000 種類の一般的な物体を分類するように設計されています。これに対して、本節では、normal-cecum、normal-pylorus、normal-z-line、esophagitis、ulcerative-colitis、polyps の 6 カテゴリの分類問題を扱います。そのため、torchvision.models モジュールから読み込んだ DenseNet 121 の出力層のユニット数を 6 に変更する必要があります。この修正作業はアーキテクチャを呼び出すたびに行う必要があり、手間がかかります。そこで、一連の処理を関数化してから利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "def densenet121(num_classes, weights=None):\n",
    "    model = torchvision.models.densenet121(weights='DEFAULT')\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = torch.nn.Linear(in_features, num_classes)\n",
    "    if weights is not None:\n",
    "        model.load_state_dict(torch.load(weights))\n",
    "    return model\n",
    "\n",
    "model = densenet121(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルが学習データを効率よく学習できるようにするため、損失関数（`criterion`）、学習アルゴリズム（`optimizer`）、学習率（`lr`）、および学習率を調整するスケジューラ（`lr_scheduler`）を設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、訓練データと検証データを読み込み、モデルが入力できる形式に整えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder('kvasir/train', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "valid_dataset = torchvision.datasets.ImageFolder('kvasir/valid', transform=transform)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備が整ったら、訓練を開始します。訓練プロセスでは、訓練と検証を交互に繰り返します。訓練では、訓練データを使ってモデルのパラメータを更新し、その際の損失（誤差）を記録します。検証では、検証データを使ってモデルの予測性能（正解率）を計算し、その結果を記録します。このサイクルを繰り返すことで、モデルの精度を少しずつ向上させていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "metric_dict = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training phase\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    n_correct_train = 0\n",
    "    n_train_samples = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs.data, 1)\n",
    "        n_correct_train += torch.sum(predicted_labels == labels).item()\n",
    "        n_train_samples += labels.size(0)\n",
    "        running_loss +=  loss.item() / len(train_loader)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "\n",
    "    # validation phase\n",
    "    model.eval()\n",
    "    \n",
    "    n_correct_valid = 0\n",
    "    n_valid_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted_labels = torch.max(outputs.data, 1)\n",
    "            n_correct_valid += torch.sum(predicted_labels == labels).item()\n",
    "            n_valid_samples += labels.size(0)\n",
    "\n",
    "    metric_dict.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': running_loss,\n",
    "        'train_acc': n_correct_train / n_train_samples,\n",
    "        'valid_acc': n_correct_valid / n_valid_samples\n",
    "    })\n",
    "  \n",
    "    print(metric_dict[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データに対する損失と検証データに対する正解率を可視化し、訓練過程を評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "metric_dict = pd.DataFrame(metric_dict)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].plot(metric_dict['epoch'], metric_dict['train_loss'])\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].set_title('Train')\n",
    "ax[1].plot(metric_dict['epoch'], metric_dict['valid_acc'])\n",
    "ax[1].set_ylim(0, 1)\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].set_title('Validation')\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "glue('valid_acc', metric_dict['valid_acc'].iloc[5:].mean(), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可視化の結果から、エポックが進むにつれて訓練データに対する損失は減少し、7 エポック以降に収束し始める傾向が見られました。また、検証データに対する正解率は、最初の数エポックで約 {glue:text}`valid_acc:.3f` に達した後、エポック数が増えてもそれ以上の向上は見られませんでした。このグラフから、数エポックの訓練だけで最適なモデルが得られることがわかります。\n",
    "\n",
    "次に、同じ手順を他の深層ニューラルネットワークアーキテクチャ（ResNet や Inception など）に対して実施し、それぞれの検証性能を比較します。そして、このデータセットに最適なアーキテクチャを選択します。ただし、本節ではモデル（アーキテクチャ）選択を行わずに、DenseNet 121 を最適なアーキテクチャとして採用し、次のステップに進みます。\n",
    "\n",
    "次のステップでは、訓練サブセットと検証サブセットを統合し、最適なアーキテクチャで最初から訓練を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!rm -rf kvasir/trainvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir kvasir/trainvalid\n",
    "!cp -r kvasir/train/* kvasir/trainvalid\n",
    "!cp -r kvasir/valid/* kvasir/trainvalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適なモデルを選択する段階で、数エポックの訓練だけでも十分に高い予測性能を獲得できたことがわかったので、ここでは訓練サブセットと検証サブセットを統合したデータに対して 5 エポックだけ訓練させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = densenet121(6)\n",
    "model.to(device)\n",
    "\n",
    "# training params\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# training data\n",
    "train_dataset = torchvision.datasets.ImageFolder('kvasir/trainvalid', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# training\n",
    "num_epochs = 5\n",
    "metric_dict = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    n_correct_train = 0\n",
    "    n_train_samples = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs.data, 1)\n",
    "        n_correct_train += torch.sum(predicted_labels == labels).item()\n",
    "        n_train_samples += labels.size(0)\n",
    "        running_loss +=  loss.item() / len(train_loader)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    metric_dict.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': running_loss,\n",
    "        'train_acc': n_correct_train / n_train_samples,\n",
    "    })\n",
    "  \n",
    "    print(metric_dict[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練が完了したら、訓練済みモデルの重みをファイルに保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')\n",
    "torch.save(model.state_dict(), 'kvasir.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適なモデルが得られたら、次にテストデータを用いてモデルを詳細に評価します。正解率だけでなく、適合率、再現率、F1 スコアなどの評価指標を計算し、モデルを総合的に評価します。まず、テストデータをモデルに入力し、その予測結果を取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = densenet121(6, 'kvasir.pth')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder('kvasir/test', transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, _labels = torch.max(outputs.data, 1)\n",
    "        #print(_labels)\n",
    "        pred_labels.extend(_labels.cpu().detach().numpy().tolist())\n",
    "        true_labels.extend(labels.cpu().detach().numpy().tolist())\n",
    "\n",
    "pred_labels = [test_dataset.classes[_] for _ in pred_labels]\n",
    "true_labels = [test_dataset.classes[_] for _ in true_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、予測結果とラベルを比較し、混同行列を作成します。これにより、間違いやすいカテゴリを特定することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sklearn.metrics.confusion_matrix(true_labels, pred_labels)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = sklearn.metrics.ConfusionMatrixDisplay(cm, display_labels=test_dataset.classes)\n",
    "cmp.plot(xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それぞれのクラスに対する適合率、再現率、F1 スコアなどは、scikit-learn ライブラリを利用して計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sklearn.metrics.classification_report(true_labels, pred_labels, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論を行う際には、訓練や評価時と同様に、torchvision.models モジュールから DenseNet 121 のアーキテクチャを読み込み、出力層のクラス数を設定します。その後、`load_state_dict` メソッドを使用して訓練済みの重みファイルをモデルにロードします。これらの処理はすでに関数化（`densenet121`）されているため、その関数を利用して簡単に実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "labels = ['esophagitis', 'normal-cecum', 'normal-pylorus', 'normal-z-line', 'polyps', 'ulcerative-colitis']\n",
    "model = densenet121(6, 'kvasir.pth')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このモデルを使って推論を行います。まず、polyps の画像を 1 枚選び、訓練時と同じ前処理を施します。その後、前処理をした画像をモデルに入力し、予測結果を表示させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'kvasir/test/polyps/18a31930-8305-49a8-8bb4-1baf35da8c3e.jpg'\n",
    "\n",
    "image = PIL.Image.open(image_path).convert('RGB')\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    score = model(input_tensor)[0]\n",
    "\n",
    "output = pd.DataFrame({\n",
    "    'class': labels,\n",
    "    'probability': torch.softmax(score, axis=0).cpu().detach().numpy() \n",
    "})\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、別の例を見てみましょう。esophagitis の画像をモデルに入力し、推論を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'kvasir/test/esophagitis/ceb61e27-08b3-4887-8bde-3c8f6c537e28.jpg'\n",
    "\n",
    "image = PIL.Image.open(image_path).convert('RGB')\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    score = model(input_tensor)[0]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'class': labels,\n",
    "    'probability': torch.softmax(score, axis=0).cpu().detach().numpy() \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類根拠の可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みニューラルネットワークを用いた画像分類では、畳み込み層で抽出された特徴マップが分類に大きな影響を与えています。そのため、最後の畳み込み層で得られた特徴マップと、それに対応する重みを可視化することで、モデルがどの部分に注目して分類を行ったのか、つまり判断の根拠を明確にすることができます。\n",
    "\n",
    "本節では、Grad-CAM（Gradient-weighted Class Activation Mapping）および Guided Grad-CAM という手法を用いて、モデルの判断根拠を可視化します。可視化には Python の grad-cam パッケージを使用します。必要に応じてインストールし、grad-cam の[チュートリアル](https://jacobgil.github.io/pytorch-gradcam-book/introduction.html)を参考にしながら、Grad-CAM および Guided Grad-CAM を計算し、可視化するための関数を定義します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(image_path):\n",
    "    # load models\n",
    "    labels = ['esophagitis', 'normal-cecum', 'normal-pylorus', 'normal-z-line', 'polyps', 'ulcerative-colitis']\n",
    "    model = torchvision.models.densenet121(weights='DEFAULT')\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = torch.nn.Linear(in_features, 6)\n",
    "    model.load_state_dict(torch.load('kvasir.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # load image\n",
    "    image = PIL.Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    rgb_img = cv2.imread(image_path, 1)[:, :, ::-1]\n",
    "    rgb_img = np.float32(np.array(SquareResize()(image))) / 255\n",
    "    \n",
    "    # Grad-CAM\n",
    "    with pytorch_grad_cam.GradCAM(model=model, target_layers=[model.features.denseblock4.denselayer16]) as cam:\n",
    "        cam.batch_size = 32\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=None,aug_smooth=True, eigen_smooth=True)\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        cam_image = pytorch_grad_cam.utils.image.show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "        prob = torch.softmax(cam.outputs[0], axis=0).cpu().detach().numpy()\n",
    "\n",
    "    gb_model = pytorch_grad_cam.GuidedBackpropReLUModel(model=model, device=device)\n",
    "    gb = gb_model(input_tensor, target_category=None)\n",
    "    cam_mask = np.stack([grayscale_cam, grayscale_cam, grayscale_cam], axis=-1)\n",
    "    cam_gb = pytorch_grad_cam.utils.image.deprocess_image(cam_mask * gb)\n",
    "    gb = pytorch_grad_cam.utils.image.deprocess_image(gb)\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = plt.subplots(2, 2)\n",
    "    ax[0, 0].imshow(rgb_img)\n",
    "    ax[0, 0].axis('off')\n",
    "    ax[0, 0].set_title('Original Image', fontsize=16)\n",
    "    ax[0, 1].imshow(cam_image)\n",
    "    ax[0, 1].axis('off')\n",
    "    ax[0, 1].set_title('Grad-CAM', fontsize=16)\n",
    "    ax[1, 0].imshow(gb)\n",
    "    ax[1, 0].axis('off')\n",
    "    ax[1, 0].set_title('Guided Backpropagation', fontsize=16)\n",
    "    ax[1, 1].imshow(cam_gb)\n",
    "    ax[1, 1].axis('off')\n",
    "    ax[1, 1].set_title('Guided Grad-CAM', fontsize=16)\n",
    "    print(pd.DataFrame({'class': labels, 'probability': prob}))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、いくつかの画像をこの可視化関数に入力し、モデルの予測結果とその判断根拠を可視化します。これにより、モデルがどの部分に注目して分類を行ったのかを視覚的に確認することができます。必要に応じて、他の画像を入力し、それぞれの分類結果と判断根拠を可視化してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz('kvasir/test/polyps/20cb9bd3-af0e-44ea-98a2-186b148d2595.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz('kvasir/test/normal-z-line/96b06b18-6250-484f-955a-6f0179db08a5.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!rm -rf kvasir/trainvalid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
