{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評価指標"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの予測性能を正確に評価するには、データ分布や正例と負例のバランスなどのデータの特性や解析の目的を考慮し、適切な評価指標を選ぶことが重要です。以下に、一般的によく使用される指標を紹介します。これらの指標は、予測結果と実際のラベルを比較して計算されます。独自に関数を定義して計算することも可能ですが、計算ミスや見落としを防ぐため、scikit-learn などの信頼性の高いライブラリを利用することをお勧めします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 回帰問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回帰モデルの性能を評価するためには、**平均二乗誤差**（**mean squared error**; **MSE**）、**二乗平均平方根誤差**（**root mean squared error**; **RMSE**）、**平均絶対誤差**（**mean absolute error**; **MAE**）、**決定係数**（**coefficient of determination**; **R<sup>2</sup>**）、**自由度調整済み決定係数**（**adjusted R<sup>2</sup>**）などが使われています。それぞれのデータの特性やモデルの目的に応じて使い分けることが重要で、どの指標を用いるかは、分析の目標やデータの性質によって変わります。とくに、生物学データの場合は、正規分布に従うことが多く、予測精度を直感的に理解しやすい RMSE が広く利用されています。\n",
    "\n",
    "\n",
    "```{index} へ 平均二乗誤差\n",
    ":name: 平均二乗誤差\n",
    "```\n",
    "\n",
    "```{index} MSE\n",
    ":name: MSE\n",
    "```\n",
    "\n",
    "\n",
    "### 平均二乗誤差（MSE）\n",
    "\n",
    "平均二乗誤差（MSE）は、観測値 $y_{i}$ と予測値 $\\hat{y}_{i}$ の残差の二乗平均で計算される指標で、次式で表されます。\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "平均二乗誤差の特徴は、残差が大きいデータ（外れ値）の影響を受けやすいことです。観測値が正規分布やそれに近い分布である場合、平均二乗誤差を評価指標として使用すると適切なモデルを得やすいとされています。\n",
    "\n",
    "ただし、観測データに外れ値が含まれる場合、平均二乗誤差は外れ値を重視しすぎる可能性があります。これは、外れ値による大きな残差が二乗され、全体の評価を大きく左右するためです。そのため、外れ値がモデル構築におけるノイズである場合には注意が必要です。一方で、生物学的な意味を持つ外れた値を含む場合には、それらを考慮したモデル構築が求められます。\n",
    "\n",
    "scikit-learn ライブラリを利用すれば、次のように計算できます。\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "```\n",
    "\n",
    "```{index} に 二乗平均平方根誤差\n",
    ":name: 二乗平均平方根誤差\n",
    "```\n",
    "\n",
    "```{index} RMSE\n",
    ":name: RMSE\n",
    "```\n",
    "\n",
    "\n",
    "### 二乗平均平方根誤差（RMSE）\n",
    "\n",
    "二乗平均平方根誤差（RMSE）は、平均二乗誤差を平方根で調整した指標で、次の式で表されます。\n",
    "\n",
    "$$\n",
    "\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "二乗平均平方根誤差の大きな特徴は、値の単位が観測値や予測値と同じであるため、予測の誤差を直感的に把握しやすい点です。しかし、二乗平均平方根誤差も外れ値に影響を受けやすい点では平均二乗誤差と同様です。\n",
    "\n",
    "scikit-learn ライブラリを用いると次のように計算できます。\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse = root_mean_squared_error(y_true, y_pred)\n",
    "```\n",
    "\n",
    "\n",
    "```{index} へ 平均絶対誤差\n",
    ":name: 平均絶対誤差\n",
    "```\n",
    "\n",
    "```{index} MAE\n",
    ":name: MAE\n",
    "```\n",
    "\n",
    "\n",
    "### 平均絶対誤差（MAE）\n",
    "\n",
    "平均絶対誤差（MAE）は、観測値と予測値の絶対誤差の平均で計算されます。\n",
    "\n",
    "$$\n",
    "\\mathrm{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "この指標は、外れ値や非正規分布のデータに対しても比較的頑健です。平均二乗誤差や二乗平均平方根誤差が外れ値の影響を大きく受けるのに対し、平均絶対誤差は単純に誤差の絶対値を平均するため、外れ値の影響が軽減されます。\n",
    "\n",
    "scikit-learn ライブラリを用いると次のように計算できます。\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "```\n",
    "\n",
    "\n",
    "```{index} け 決定係数\n",
    ":name: 決定係数\n",
    "```\n",
    "\n",
    "```{index} R-squared\n",
    ":name: R-squared\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 決定係数（R<sup>2</sup>）\n",
    "\n",
    "決定係数は、特徴量が目的変数をどれだけ説明できているかを示す指標で、次の式で計算されます。\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "$R^2$ の値は 0 ～ 1 の範囲を取るのが一般的で、1 に近いほどモデルの予測が正確であることを示します。ただし、データの分布やモデルに含まれるパラメータ数によっては、$R^2$ が負の値になることもあります。例えば、三次関数のような複雑なデータを一次関数で近似しようとすると、予測がランダムな場合よりも精度が悪くなる可能性があり、この場合 $R^2$ が負になります。\n",
    "\n",
    "scikit-learn ライブラリを用いると次のように計算できます。\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "```\n",
    "\n",
    "\n",
    "```{index} じ 自由度調整済み決定係数\n",
    ":name: 自由度調整済み決定係数\n",
    "```\n",
    "\n",
    "```{index} adjusted R-squared\n",
    ":name: adjusted R-squared\n",
    "```\n",
    "\n",
    "なお、特徴量の数が増えると、過剰適合によって $R^2$ の値が不当に高くなることがあります。そのため、モデルの複雑さを考慮した **自由度調整済み決定係数**（**Adjusted R<sup>2</sup>**）がよく使われます。\n",
    "\n",
    "\n",
    "### 自由度調整済み決定係数（Adjusted R<sup>2</sup>）\n",
    "\n",
    "自由度調整済み決定係数は、特徴量の数を考慮して決定係数を補正した指標です。式は次の通りです。\n",
    "\n",
    "$$\n",
    "R^{\\prime 2}=1-\\frac{\\frac{1}{n-p-1}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{\\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\bar{y})^2}\n",
    "$$\n",
    "\n",
    "ここで、$n$ はサンプル数、$p$ は特徴量の数です。Adjusted R<sup>2</sup> を用いることで、特徴量が増えても、モデルの過剰適合を防ぎつつ適切な評価が可能になります。\n",
    "\n",
    "scikit-learn ライブラリを用いると次のように計算できます。\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "n = x.shape[0]  # サンプル数\n",
    "p = x.shape[1]  # 特徴量数\n",
    "adj_r2 = 1 - (1 - r2) * (n - 1)/(n - p - 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{index} こ 混同行列\n",
    ":name: 混同行列\n",
    "```\n",
    "\n",
    "### 混同行列\n",
    "\n",
    "分類問題を評価するために、観測値と予測結果から作られる**混同行列**（**confusion matrix**）を作成し、それを元にさまざまな評価指標を計算します。混同行列は、陽性（あるいは正例）と観測されるサンプルを正しく陽性と分類した **true positive**（**TP**）と間違って陰性と分類した **false negative**（**FN**）、陰性と観測されるサンプルを正しく陰性と分類した **true negative**（**TN**）と間違って陽性と分類した **false positive**（**FP**）から構成されます。\n",
    "\n",
    "|       |                     | 予測結果                  |                          |\n",
    "|-------|---------------------|--------------------------|--------------------------|\n",
    "|       |                     | $\\hat{y} = 1$ (positive) | $\\hat{y} = 0$ (negative) |\n",
    "| 観測値 | $y = 1$ (positive)  | true positive (TP)       | false negative (FN)      |\n",
    "|       | $y = 0$ (negative)  | false positive (FP)      | true negative (TN)       |\n",
    "\n",
    "\n",
    "scikit-learn ライブラリを利用すると、混同行列を簡単に計算することができます。計算された混同行列は NumPy の二次元配列として保存されます。\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "```\n",
    "\n",
    "また、scikit-learn ライブラリの `ConfusionMatrixDisplay` 関数を利用することで、混同行列をヒートマップで可視化することもできます。\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay(cm)\n",
    "```\n",
    "\n",
    "```{index} せ 正解率\n",
    ":name: 正解率\n",
    "```\n",
    "\n",
    "```{index} accuracy\n",
    ":name: accuracy\n",
    "```\n",
    "\n",
    "### 正解率（accuracy）\n",
    "\n",
    "**正解率**（**accuracy**）は、全サンプルのうち、正しく予測されたデータの割合を示す指標です。次の式で計算されます。\n",
    "\n",
    "$$\n",
    "\\mathrm{ACC} = \\frac{TP + TN}{TP + TN + FN + FP}\n",
    "$$\n",
    "\n",
    "正解率が 1.0 に近いほどモデルの精度が高いと判断されます。しかし、陽性と陰性のサンプル数が大きく異なる場合、正解率は適切な評価指標とならないことがあります。たとえば、陽性サンプルが全体の 90% を占めるデータセットでは、すべてを陽性と予測するだけで正解率が 90% となります。こうした場合には、他の指標（適合率や再現率など）を補完的に用いる必要があります。\n",
    "\n",
    "\n",
    "scikit-learn ライブラリを利用すると、次のように計算できます。\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "```\n",
    "\n",
    "```{index} と 特異度\n",
    ":name: 特異度\n",
    "```\n",
    "\n",
    "```{index} specificity\n",
    ":name: specificity\n",
    "```\n",
    "\n",
    "```{index} し 真陰性率\n",
    ":name: 真陰性率\n",
    "```\n",
    "\n",
    "```{index} true negative rate\n",
    ":name: true negative rate\n",
    "```\n",
    "\n",
    "### 特異度（specificity）\n",
    "\n",
    "**特異度**（**specificity**）は、真陰性率（true negative rate）とも呼ばれ、陰性サンプルをどれだけ正しく陰性と予測できたかを示す指標です。次の式で計算されます。\n",
    "\n",
    "$$\n",
    "\\mathrm{specificity} = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "モデルの偽陽性を減らすしたい場合に、特異度が適切な評価指標の一つと言えます。\n",
    "\n",
    "\n",
    "scikit-learn ライブラリでは特異度を直接計算する関数が用意されていません。そのため、混同行列の値を利用して次のように計算します。\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred)\n",
    "spe = tn / (tn + fp)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{index} て 適合率\n",
    ":name: 適合率\n",
    "```\n",
    "\n",
    "```{index} precision\n",
    ":name: precision\n",
    "```\n",
    "\n",
    "### 適合率（precision）\n",
    "\n",
    "**適合率**（**precision**）は、陽性と予測されたサンプルの中で、実際に陽性であるものの割合を示します。次の式で計算されます。\n",
    "\n",
    "$$\n",
    "\\mathrm{precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "適合率が高いほど、陽性と予測されたデータの信頼性が高いことを意味します。ただし、適合率を重視しすぎると、陽性とする基準が厳しくなり、実際に陽性であるサンプルの取りこぼし（FN）が発生するリスクがあります。\n",
    "\n",
    "\n",
    "scikit-learn ライブラリを利用すると、次のように計算できます。\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_score\n",
    "pre = precision_score(y_true, y_pred)\n",
    "```\n",
    "\n",
    "\n",
    "```{index} さ 再現率\n",
    ":name: 再現率\n",
    "```\n",
    "\n",
    "```{index} recall\n",
    ":name: recall\n",
    "```\n",
    "\n",
    "```{index} し 真陽性率\n",
    ":name: 真陽性率\n",
    "```\n",
    "\n",
    "```{index} true positive rate\n",
    ":name: true positive rate\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 再現率（recall）\n",
    "\n",
    "**再現率**（**recall**）は感度、真陽性率（true positive rate）とも呼ばれ、陽性サンプルをどれだけ正しく陽性と予測できたかを示す指標です。次の式で計算されます。\n",
    "\n",
    "$$\n",
    "\\mathrm{recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "再現率が高いモデルは、陽性サンプルを見逃す可能性が低くなります。ただし、再現率を重視しすぎると、陰性サンプルを誤って陽性と予測する（FP）が増えやすくなり、その結果、適合率が低下する可能性があります。\n",
    "\n",
    "scikit-learn ライブラリを利用すると、次のように計算できます。\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import recall_score\n",
    "rec = recall_score(y_true, y_pred)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{index} F1 スコア\n",
    ":name: F1 スコア\n",
    "```\n",
    "\n",
    "```{index} F1 score\n",
    ":name: F1 score\n",
    "```\n",
    "\n",
    "### F1 スコア（F1 score）\n",
    "\n",
    "**F1 スコア**（**F1 score**）は、適合率と再現率の調和平均を取った指標です。次のように計算されます。\n",
    "\n",
    "$$\n",
    "\\mathrm{F1} = 2 \\frac{\\mathrm{precision} \\times \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}\n",
    "$$\n",
    "\n",
    "F1 スコアは、適合率と再現率のバランスを取りたい場合に利用されます。特に、陽性と陰性のサンプル数に偏りがあるデータセットでは、F1 スコアを用いることで、モデルの性能をより適切に評価できます。\n",
    "\n",
    "scikit-learn ライブラリを利用すると、次のように計算できます。\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "```\n",
    "\n",
    "\n",
    "```{index} ROC曲線\n",
    ":name: ROC曲線\n",
    "```\n",
    "\n",
    "```{index} AUC\n",
    ":name: AUC\n",
    "```\n",
    "\n",
    "```{index} PR曲線\n",
    ":name: PR曲線\n",
    "```\n",
    "\n",
    "### その他の指標\n",
    "\n",
    "分類問題におけるモデル性能を評価するために、他にも ROC 曲線や AUC、PR 曲線などの指標があります。これらは、問題の性質や解析の目的に応じて適切に利用してください。\n",
    "\n",
    "**ROC 曲線**（**receiver operating characteristic curve**; **ROC curve**）は、分類モデルの再現率と特異度の関係を視覚化したものです。予測スコアの閾値を変化させた際のモデルの性能を確認できます。また、ROC 曲線の下の面積である **AUC**（**area under the curve**）を指標として使用することも一般的です。AUC は 0 から 1 の値を取り、1 に近いほど分類性能が高いことを示し、0.5 前後の値は分類がランダムであることを示します。\n",
    "\n",
    "**PR 曲線**（**precision-recall curve**）は、分類モデルの適合率と再現率の関係を視覚化したものです。特にクラスが不均衡なデータセットの場合に有用です。また、ROC 曲線と同様に、PR 曲線の下の面積を指標として評価する場合もあります。\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
